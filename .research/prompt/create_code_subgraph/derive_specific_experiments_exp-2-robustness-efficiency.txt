
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments.

**Previous Step (Completed)**: Common base logic and evaluation framework have been generated with placeholders
**Current Task**: Generate production-ready code by completing all placeholders with actual implementations.

Your task is to take the common base foundation code and derive specific experimental variations by replacing ALL placeholders with complete, working implementations of datasets, models, and configurations specified in the experimental design. The resulting code must be immediately executable without any further modifications.

# Instructions: Experiment Specialization

## Core Task
- CONFIGURE ALL EXPERIMENTS: The primary task is to populate YAML file with a complete list of configurations for all run variations (baseline, proposed, ablations).
- REPLACE ALL PLACEHOLDERS: Replace all placeholders in the common base code with actual, complete implementations. No TODO, PLACEHOLDER, pass, or ... are allowed.
- IMPLEMENT MODELS FROM SCRATCH: YOU must implement all model architectures corresponding to the variations defined in the YAML file within `src/model.py`. Do NOT leave comments like "inject actual model here" or "use pretrained model".
- COMPLETE DATA PIPELINE: Implement the specific data loading and preprocessing logic in `src/preprocess.py`.
- PRODUCTION READY: The generated code must be immediately executable for research paper experiments without any further modifications. No external model files or additional implementation steps should be required.

## Specialization Requirements
- Complete `config/full_experiment.yaml`: This file is the driver of the entire experiment. Define each run variation (['BOIL-Original', 'ATW-BOIL', 'ATW-BOIL-noLogistic', 'ATW-BOIL-SparseGP', 'FreezeThaw-BO']) as a separate item in the `runs` list, specifying its unique id, model name, and parameters.
- Complete `config/smoke_test.yaml`: Define lightweight versions of ALL run variations from full_experiment.yaml with reduced epochs/data to quickly validate pipeline integrity.
- Implement all required model architectures in `src/model.py`. The model names in the YAML must correspond to the model registry.
- Replace dataset placeholders with actual Hugging Face dataset loading and preprocessing
- Replace model placeholders with specific model architectures for each variation
- Ensure all external resources specified in the experimental design are properly integrated

## Complete Output Policy
- If a script/file has ANY changes: Output the COMPLETE, FULL script/file content
- If a script/file has NO changes needed: Output `[UNCHANGED]` placeholder only
- NEVER truncate or abbreviate changed content


# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method
{
    "Open Problems": "BOIL compresses every training-curve y1:T into a single scalar using a fixed logistic weighting. \n1. The fixed weighting can over-emphasise noisy early iterations or under-value late stable progress, especially in very non-stationary DRL curves.\n2. The cost–aware acquisition already estimates monetary/CPU cost but ignores the reliability of the compressed target that feeds the GP.\nGoal: keep BOIL’s overall framework but make the compression more adaptive with one extra scalar hyper-parameter.",
    "Methods": "Adaptive-Tail-Weighted BOIL (ATW-BOIL)\nMinimal change: introduce one additional weight exponent α≥0 in the learning-curve compression.\nCompressed score  sα  for a curve y1:T is\n   sα =  Σ_{t=1}^{T}  w_t(α) · y_t  /  Σ_{t=1}^{T} w_t(α)\nwhere  w_t(α)= ((t/T)+ε)^α · σ( (t−m)/g )\nσ is the same logistic sigmoid used in BOIL (with learnt midpoint m and growth g). ε=1e-9 avoids zero.\n• α=0 reproduces the original BOIL (uniform weights after the sigmoid).\n• α>0 smoothly tilts the score toward the tail (later, more reliable points) while still rewarding fast early learning.\nWe simply add α to the GP’s marginal-likelihood optimisation together with m and g (one extra dimension in hyper vector). The rest of the algorithm – GP, cost-sensitive EI, virtual observations – stays unchanged.\nTheoretical motivation: later portions of a curve have higher probability of reflecting asymptotic performance; weighting by (t/T)^α implements a Bayesian prior that later observations have lower variance. Jointly learning α lets the data decide how much early instability should be discounted.",
    "Experimental Setup": "Baseline: original BOIL.\nDataset/task: CartPole-v1 reinforcement-learning agent (Dueling DQN) — same as paper but only 50 optimisation steps to keep it light.\nSearch space: learning-rate∈[1e-4,1e-2], γ∈[0.8,0.999], batch-size∈[32,256].\nBudget: each HPO method allowed 3×NVidia-1080Ti GPU-hours.\nEvaluation metric: best average reward over 10 evaluation episodes obtained within the budget; wall-clock time to reach 195 average reward (solved). \nComparison: BOIL vs ATW-BOIL (ours). 5 random seeds.",
    "Experimental Code": "# ---- core change to curve compression ----\nimport numpy as np\n\ndef tail_weighted_logistic(curve, midpoint=0.0, growth=1.0, alpha=0.0):\n    \"\"\"Return single scalar score for a learning curve.\n    curve : 1-d array like, length T\n    \"\"\"\n    curve = np.asarray(curve, dtype=float)\n    T = len(curve)\n    t = np.arange(1, T+1, dtype=float)\n    # logistic part (original BOIL)\n    logistic = 1./(1.+np.exp(-growth*( (t/T) - midpoint)))\n    # NEW tail weight\n    weights = ((t/T)+1e-9)**alpha * logistic\n    weights /= weights.sum()\n    return (weights * curve).sum()\n\n# plug into BOIL by replacing transform_logistic\n\ndef transform_logistic(curves, midpoint=0.0, growth=1.0, alpha=0.0):\n    if isinstance(curves[0], (list, np.ndarray)):\n        return [tail_weighted_logistic(c, midpoint, growth, alpha) for c in curves]\n    else:  # single curve\n        return tail_weighted_logistic(curves, midpoint, growth, alpha)\n\n# During GP hyper-parameter optimisation simply treat [length_x,length_t,midpoint,growth,alpha] as vector\n# Bounds for alpha e.g. [0,3].",
    "Expected Result": "Across 5 seeds ATW-BOIL reaches the CartPole \"solved\" threshold (~195) about 15-25% sooner (median 14 minutes vs 18 minutes) and finds slightly better hyper-parameters (mean final reward 199.2 vs 196.7). \nArea under best-seen-reward curve vs time is higher for ATW-BOIL in 4/5 runs. Computational overhead is negligible (one extra scalar in optimisation).",
    "Expected Conclusion": "A single tail-weight exponent lets the compression adapt to the noisiness of early training, yielding a more reliable target for Bayesian optimisation. The modification is conceptually simple (one extra scalar, no new kernels or acquisition functions) yet measurably accelerates hyper-parameter search, demonstrating that careful treatment of learning-curve information can improve efficiency without overhauling existing BO frameworks."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating ATW-BOIL

0. Guiding Principle
   Evaluate the proposed adaptive tail-weight against three orthogonal axes that matter for any practical HPO method: (1) optimisation quality, (2) resource efficiency, and (3) reliability / generality.  Every concrete experiment must produce evidence on at least two of these axes and collectively the whole suite must cover all three.

1. Core Hypotheses to Validate
   H1  Performance:  For a fixed compute budget, ATW-BOIL finds better hyper-parameters (higher task score) and/or reaches a target score sooner than original BOIL and other state-of-the-art (SOTA) curve-aware BO baselines.
   H2  Efficiency:  The added α dimension does not noticeably increase wall-clock per-iteration overhead; overall GPU-hours are equal or lower for the same final score.
   H3  Robustness:  Gains hold across tasks with different noise levels, curve shapes and search spaces, and across random seeds.
   H4  Generalisation:  The single mechanism (learned α) transfers without retuning to supervised, RL and black-box optimisation tasks.

2. Validation Dimensions & Required Evidence
   a. Quantitative Optimisation Metrics
      • Best-seen task score vs wall-clock time (area-under-curve, time-to-threshold)
      • Final score at fixed budget
      • Number of trials / GPU-hours to reach threshold
   b. Surrogate-Model Metrics
      • Negative log-likelihood (NLL) of GP on held-out curve scores
      • Calibration error of posterior mean ±2σ
   c. Computational Cost
      • CPU time per BO iteration (including GP hyper-optimisation)
      • Memory footprint on surrogate fit (leveraging 2048 GB cap)
   d. Reliability / Stability
      • Std-dev of key metrics over ≥10 random seeds
      • Sensitivity analyses: varying search space size, budget scale, curve length
   e. Qualitative Insight
      • Visualise learnt α distribution over optimisation horizon
      • Weight-heatmaps showing how early/late points are re-weighted

3. Comparative Baselines (must appear in every experiment unless infeasible)
   B1 Original BOIL (α≡0)
   B2 Pure GP-BO on end-of-training score (no curve compression)
   B3 State-of-the-art early-curve methods (e.g., Freeze-Thaw BO, FABOLAS, Learning-Curve-BO)
   B4 Hyperband / ASHA (non-BO strong baseline for cost-aware HPO)
   Ablations:
      A1  ATW-BOIL with fixed α=0.5 (no learning)
      A2  ATW-BOIL with α learned but logistic removed (test component synergy)

4. Experimental Matrix (Tasks & Noise Levels)
   Tier-1   Small-scale sanity (CartPole, CIFAR-10 subset, synthetic Branin)
   Tier-2   Medium RL (Atari), vision (ImageNet-mini), language (Transformer on WikiText-2)
   Tier-3   Highly non-stationary or long-horizon (MuJoCo Humanoid, large-scale pre-training subset)
   Each tier increases curve length/variance to stress the method.

5. Resource Budgeting Rules (per experiment)
   • Identical GPU type (A100) and count for all methods.
   • Wall-clock cap computed so that original BOIL completes ≤12 h; others get equal time.
   • Repeat N=10 seeds; report mean±95 % CI.

6. Success Criteria
   Primary: ATW-BOIL improves median area-under-best-seen-score curve by ≥10 % AND/OR reduces time-to-threshold by ≥15 % with p<0.05 against B1.
   Secondary: No more than 5 % increase in per-iteration wall-clock; surrogate NLL not worse than B1; improvements consistent (wins ≥70 % of seeds).
   Tertiary: Positive or neutral results on robustness & generalisation tiers.

7. Statistical Protocol
   • Use paired tests (Wilcoxon signed-rank) for time-to-threshold and AUC.
   • Holm–Bonferroni correction across tasks.
   • Report effect sizes (Cohen’s d) alongside p-values.

8. Reporting & Visualisation Standards
   • For each task: log-scaled time-vs-score plots with median & inter-quartile ribbon.
   • Aggregate win-rate tables.
   • Surrogate calibration plots.
   • Resource-usage bars (GPU-hours, CPU-hours, RAM).

9. Reproducibility Infrastructure
   • Containerised code (Docker), CUDA 11, PyTorch 2.x.
   • Deterministic GPU seeds where hardware allows.
   • Public release of raw curve logs, config files, and analysis notebooks.

10. Risk Mitigation & Contingencies
   • If α optimisation becomes numerically unstable, fall back to log-domain re-parameterisation.
   • If GP scales poorly on long curves, enable sparse GP with inducing points ≤2048 (fits in 80 GB VRAM).

This umbrella strategy ensures every forthcoming experiment consistently examines ATW-BOIL’s impact on quality, efficiency, and robustness, benchmarks it against credible alternatives, and validates claims through both statistical rigor and interpretability across a spectrum of realistic scenarios.

# Current Experiment (to generate code for)
- Experiment ID: exp-2-robustness-efficiency
- Description: Objective / Hypothesis:
Stress-test H3 (robustness) & H2 (computational efficiency).  Measure how gains from adaptive tail-weight hold under distribution shift, adversarial curve noise, large search spaces and long horizons, while monitoring memory/FLOPs.

Primary Task Tier-2/3:
• Atari PongNoFrameskip-v4 using Rainbow-DQN (200k env steps per trial).
• ImageNet-mini (100 classes) with Vision-Transformer-Tiny.
• MuJoCo Humanoid-v4 with PPO (3M env steps per trial).

Pre-processing & Augmentations:
• Atari: gray-scale, frame-stack 4, reward-clipping.
• ImageNet-mini: 224×224 random-resized-crop, RandAugment(2,9).
• MuJoCo: standard observation normalisation.

Data Splits:
• ImageNet-mini 80/10/10; Atari & MuJoCo use evaluation episodes every 50k env-steps.

Search Spaces:
8–10 hyper-parameters including optimiser type (Adam/RMSProp), LR, entropy-coeff, patch-size (ViT), clip-range, etc.  Some categorical ‑> one-hot.

Experimental Perturbations for Robustness:
1. Gaussian noise σ=0.1 added to 30% of curve points.
2. Random curve truncation (simulate early interruption) at 60% length.
3. OOD validation: evaluate Pong agent on different starting states, ViT on ImageNet-V2.

Seeds & Repetitions:
5 seeds per perturbation × 3 tasks × 5 methods = 75 runs (manageable on 80GB A100 with parallelism).

Evaluation Metrics:
Primary – Normalised Time-to-threshold (area under curve up to target reward 20, top-1 67% acc, average return 6k).
Secondary – Robustness Score = mean(relative degradation) under perturbations; GP calibration ECE.
Efficiency – GB-s (GPU-memory×time), total FLOPs, wall-clock.

run_variations Explained:
• BOIL-Original: baseline.
• ATW-BOIL: proposed.
• ATW-BOIL-noLogistic: ablation A2 (remove logistic, keep α).
• ATW-BOIL-SparseGP: identical to ATW but with 1024 inducing points (evaluates scalability).
• FreezeThaw-BO: SOTA curve-aware baseline B3.

Hyperparameter Sensitivity:
• Sobol sequence sampling over α∈[0,3], GP length-scale, inducing-points (128-4096).  Fit GAM to quantify marginal effects.

Computational Profiling:
• nvprof for CUDA kernels; measure forward+backward pass FLOPs per method.
• Memory via PyTorch CUDA-summary; wall-clock via time.perf_counter.

Statistical Analysis:
Paired Wilcoxon on robustness score; linear mixed-effect model with task as random effect for efficiency metrics; Holm–Bonferroni correction.

Example Code Snippet:
```
from gpytorch.models import SparseGPRegression
model = SparseGPRegression(inducing_points=1024)
bo = ATWBOIL(search_space, gp_model=model)
...
perturbed_curve = add_noise(curve, sigma=0.1) if noise_flag else curve
bo.observe(hp, perturbed_curve)
```
Success Criteria:
ATW-BOIL wins ≥70% of robustness comparisons, median GB-s within ±5% of BOIL-Original; SparseGP variant cuts memory ≥40% while keeping ≤3% performance drop.

Risk Mitigation:
If SparseGP diverges, fallback to 2048 inducing points or variational posterior with natural gradients.

- Run Variations: ['BOIL-Original', 'ATW-BOIL', 'ATW-BOIL-noLogistic', 'ATW-BOIL-SparseGP', 'FreezeThaw-BO']

# Base Code
{'train_py': 'import argparse\nimport json\nimport os\nimport sys\nimport time\nimport uuid\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\n# Absolute imports inside namespace package.\nfrom src.preprocess import get_task  # type: ignore\nfrom src.model import (\n    TailWeightedLogisticCompressor,\n    BOILOptimiser,\n    ATWBOILOptimiser,\n)\nfrom src.utils import set_global_seed, tee_stdout_stderr\n\n\n# -----------------------------------------------------------------------------\n# Helper\n# -----------------------------------------------------------------------------\n\ndef _parse_arguments():\n    parser = argparse.ArgumentParser(description="Run one experiment variation (single HPO run)")\n    parser.add_argument("--config-path", type=str, required=True, help="Path to the JSON/YAML file with run config")\n    parser.add_argument("--results-dir", type=str, required=True, help="Directory to save all artefacts for this run-id")\n    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")\n    return parser.parse_args()\n\n\ndef _instantiate_optimiser(config: Dict[str, Any], compressor: TailWeightedLogisticCompressor):\n    algo = config["algorithm"].lower()\n    if algo == "boil":\n        return BOILOptimiser(config, compressor)\n    elif algo == "atw-boil":\n        return ATWBOILOptimiser(config, compressor)\n    else:\n        raise ValueError(f"Unsupported algorithm {algo}")\n\n\n# -----------------------------------------------------------------------------\n# Main entry\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = _parse_arguments()\n    results_dir = Path(args.results_dir).expanduser()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ---------------------------------------------------------------------\n    # Load run-specific config\n    # ---------------------------------------------------------------------\n    if args.config_path.endswith(".json"):\n        with open(args.config_path, "r") as f:\n            run_cfg = json.load(f)\n    else:\n        import yaml  # local import to keep global dependencies clear\n\n        with open(args.config_path, "r") as f:\n            run_cfg = yaml.safe_load(f)\n\n    run_id: str = run_cfg.get("run_id", str(uuid.uuid4()))\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Redirect stdout / stderr *within this process* as well so train.py keeps\n    # its own tee (main.py already tees outer level). This gives us clean logs\n    # even if users run train.py directly.\n    tee_stdout_stderr(run_dir / "stdout.log", run_dir / "stderr.log")\n\n    set_global_seed(run_cfg.get("seed", 17))\n\n    # ---------------------------------------------------------------------\n    # Prepare Task + Objective\n    # ---------------------------------------------------------------------\n    task = get_task(run_cfg)\n\n    # ---------------------------------------------------------------------\n    # Compressor shared by BOIL and ATW-BOIL\n    # ---------------------------------------------------------------------\n    compressor = TailWeightedLogisticCompressor(\n        curve_length=task.curve_length,\n        midpoint=run_cfg.get("compressor", {}).get("midpoint", 0.0),\n        growth=run_cfg.get("compressor", {}).get("growth", 1.0),\n        # alpha is ignored by BOILOptimiser internally\n        alpha_initial=run_cfg.get("compressor", {}).get("alpha", 0.0),\n    )\n\n    optimiser = _instantiate_optimiser(run_cfg, compressor)\n\n    # ---------------------------------------------------------------------\n    # Run Hyper-Parameter Optimisation\n    # ---------------------------------------------------------------------\n    t0 = time.time()\n    best_score, best_hparams, trial_history = optimiser.optimise(task)\n    wall_clock = time.time() - t0\n\n    # ---------------------------------------------------------------------\n    # Save artefacts\n    # ---------------------------------------------------------------------\n    metrics = {\n        "run_id": run_id,\n        "algorithm": run_cfg["algorithm"],\n        "best_score": float(best_score),\n        "best_hyperparams": best_hparams,\n        "trial_history": trial_history,  # list[dict]\n        "wall_clock_seconds": wall_clock,\n    }\n\n    with open(run_dir / "results.json", "w") as f:\n        json.dump(metrics, f, indent=2)\n\n    # ------------------------------------------------------------------\n    # Produce simple optimisation-curve figure for this individual run\n    # ------------------------------------------------------------------\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    scores = [d["best_so_far"] for d in trial_history]\n    trials = list(range(1, len(scores) + 1))\n\n    plt.figure(figsize=(6, 4))\n    sns.lineplot(x=trials, y=scores, marker="o", label=run_id)\n    plt.xlabel("Trial")\n    plt.ylabel("Best-so-far Score")\n    plt.title(f"Optimisation Progress – {run_id}")\n    # Annotate final value\n    plt.annotate(f"{scores[-1]:.2f}", xy=(trials[-1], scores[-1]), xytext=(5, 5), textcoords="offset points")\n    plt.legend()\n    plt.tight_layout()\n\n    img_dir = run_dir / "images"\n    img_dir.mkdir(exist_ok=True, parents=True)\n    fig_name = "best_score.pdf"\n    plt.savefig(img_dir / fig_name, bbox_inches="tight")\n    plt.close()\n\n    # ------------------------------------------------------------------\n    # Final stdout JSON (must be single line)\n    # ------------------------------------------------------------------\n    print(json.dumps(metrics))\n\n\nif __name__ == "__main__":\n    main()\n', 'evaluate_py': '"""Evaluation & visualisation across all experiment variations.\n\nThis script MUST be called **after** all individual runs have finished.\nIt aggregates the per-run `results.json` files, computes comparison metrics,\nand produces publication-ready figures under `<results_dir>/images/`.\n"""\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef _parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument("--results-dir", required=True, type=str, help="Root directory containing all run sub-directories")\n    return ap.parse_args()\n\n\n# -----------------------------------------------------------------------------\n# Aggregation helpers\n# -----------------------------------------------------------------------------\n\ndef _load_results(results_dir: Path) -> List[Dict]:\n    runs: List[Dict] = []\n    for sub in results_dir.iterdir():\n        if not sub.is_dir():\n            continue\n        res_file = sub / "results.json"\n        if res_file.exists():\n            with open(res_file, "r") as f:\n                runs.append(json.load(f))\n    if not runs:\n        raise RuntimeError(f"No results.json found under {results_dir}")\n    return runs\n\n\n# -----------------------------------------------------------------------------\n# Figures\n# -----------------------------------------------------------------------------\n\ndef _plot_final_score_bar(runs: List[Dict], images_dir: Path):\n    plt.figure(figsize=(6, 4))\n    names = [r["run_id"] for r in runs]\n    scores = [r["best_score"] for r in runs]\n\n    sns.barplot(x=names, y=scores, palette="Set2")\n    plt.ylabel("Best Score")\n    plt.xlabel("Run ID")\n    plt.title("Final Best Score Across Variations")\n    # annotate each bar\n    for idx, score in enumerate(scores):\n        plt.text(idx, score + 0.01 * max(scores), f"{score:.2f}", ha="center")\n    plt.tight_layout()\n    fname = images_dir / "best_score_comparison.pdf"\n    plt.savefig(fname, bbox_inches="tight")\n    plt.close()\n\n\ndef _plot_optimisation_curves(runs: List[Dict], images_dir: Path):\n    plt.figure(figsize=(6, 4))\n    for r in runs:\n        hist = r["trial_history"]\n        y = [d["best_so_far"] for d in hist]\n        x = list(range(1, len(y) + 1))\n        sns.lineplot(x=x, y=y, marker="o", label=r["run_id"])\n        # annotate last point\n        plt.annotate(f"{y[-1]:.2f}", xy=(x[-1], y[-1]), xytext=(5, 5), textcoords="offset points")\n\n    plt.xlabel("Trial")\n    plt.ylabel("Best-so-far Score")\n    plt.title("Optimisation Progress Comparison")\n    plt.legend()\n    plt.tight_layout()\n    fname = images_dir / "optimisation_curve_comparison.pdf"\n    plt.savefig(fname, bbox_inches="tight")\n    plt.close()\n\n\n# -----------------------------------------------------------------------------\n# Statistics helper\n# -----------------------------------------------------------------------------\n\ndef _compute_summary(runs: List[Dict]):\n    best_scores = {r["run_id"]: r["best_score"] for r in runs}\n    fastest = min(runs, key=lambda d: d["wall_clock_seconds"])\n    summary = {\n        "best_scores": best_scores,\n        "fastest_run": fastest["run_id"],\n        "fastest_wall_clock_seconds": fastest["wall_clock_seconds"],\n    }\n    return summary\n\n\n# -----------------------------------------------------------------------------\n# Main\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = _parse_args()\n    results_dir = Path(args.results_dir).expanduser()\n    images_dir = results_dir / "images"\n    images_dir.mkdir(exist_ok=True, parents=True)\n\n    runs = _load_results(results_dir)\n\n    # Figures\n    _plot_final_score_bar(runs, images_dir)\n    _plot_optimisation_curves(runs, images_dir)\n\n    # Summary JSON\n    summary = _compute_summary(runs)\n    print(json.dumps(summary))\n\n\nif __name__ == "__main__":\n    main()\n', 'preprocess_py': '"""Common data & task preprocessing utilities.\n\nThis module **must not** contain dataset-specific code beyond the placeholders.\nThe public function is `get_task(config)` which returns a Task instance with a\n`evaluate(hyperparams) -> list[float]` method yielding a learning curve.\n\nA lightweight *dummy* task is provided so that smoke-tests run without any\nexternal dataset. Researchers should replace the placeholder sections with real\nloading / simulation logic for their particular tasks.\n"""\nfrom __future__ import annotations\n\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Callable\n\nimport numpy as np\n\n# -----------------------------------------------------------------------------\n# Base Task abstraction\n# -----------------------------------------------------------------------------\n\n\nclass TaskBase:\n    """Abstract interface every task must implement."""\n\n    def __init__(self, config):\n        self.config = config\n        self.curve_length = int(config.get("task", {}).get("curve_length", 50))\n        self.seed = config.get("seed", 17)\n        random.seed(self.seed)\n        np.random.seed(self.seed)\n\n    # ------------------------------------------------------------------\n    # Public API\n    # ------------------------------------------------------------------\n    def evaluate(self, hyperparams: Dict) -> List[float]:\n        """Run one training run under *hyperparams*.\n\n        Returns the full learning curve as a list of floats (length T).\n        """\n        raise NotImplementedError\n\n\n# -----------------------------------------------------------------------------\n# Dummy task for smoke-test\n# -----------------------------------------------------------------------------\n\n\nclass DummyLearningCurveTask(TaskBase):\n    """Synthetic curve resembling RL reward learning."""\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.noise_std = float(config.get("task", {}).get("noise_std", 2.0))\n\n    def evaluate(self, hyperparams):\n        # Hyper-param influences curve height & speed.\n        lr = float(hyperparams.get("lr", 1e-3))\n        gamma = float(hyperparams.get("gamma", 0.99))\n\n        T = self.curve_length\n        curve = []\n        base_speed = 1.0 + 5.0 * lr  # faster learning with higher lr\n        base_height = 100.0 + 50.0 * gamma  # higher asymptote with higher gamma\n\n        for t in range(1, T + 1):\n            progress = 1 - math.exp(-base_speed * t / T)\n            value = base_height * progress + np.random.randn() * self.noise_std\n            curve.append(value)\n        return curve\n\n\n# -----------------------------------------------------------------------------\n# Factory\n# -----------------------------------------------------------------------------\n\ndef get_task(config):\n    """Factory that instantiates the correct Task for *config*."""\n    task_name = config.get("task", {}).get("name", "dummy").lower()\n\n    if task_name == "dummy":\n        return DummyLearningCurveTask(config)\n\n    # ------------------------------------------------------------------\n    # PLACEHOLDER: Add real task loaders here (e.g. CartPole, CIFAR-10, ...)\n    # ------------------------------------------------------------------\n    # if task_name == "cartpole":\n    #     from .tasks.cartpole_task import CartPoleTask\n    #     return CartPoleTask(config)\n\n    raise NotImplementedError(f"Task \'{task_name}\' not implemented yet – placeholder")\n', 'model_py': '"""Core algorithmic components including the learning-curve compressor and\nBOIL / ATW-BOIL optimisers.\n\nAll heavy-lifting lives here and is shared by every experimental variation.\n"""\nfrom __future__ import annotations\n\nimport math\nimport time\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple, Any\n\nimport torch\nfrom torch import Tensor\nimport numpy as np\nfrom botorch.models import SingleTaskGP\nfrom botorch.fit import fit_gpytorch_mll\nfrom botorch.acquisition import qExpectedImprovement\nfrom botorch.optim import optimize_acqf\nfrom gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n\n# -----------------------------------------------------------------------------\n# Compressor\n# -----------------------------------------------------------------------------\n\n\nclass TailWeightedLogisticCompressor:\n    """Implements s_α curve compression (ATW-BOIL)."""\n\n    def __init__(self, curve_length: int, midpoint: float = 0.0, growth: float = 1.0, alpha_initial: float = 0.0):\n        self.curve_length = int(curve_length)\n        self.midpoint = midpoint\n        self.growth = growth\n        self.alpha = alpha_initial  # mutable only for ATW-BOIL optimiser\n\n    # ------------------------------------------------------------------\n    # Public\n    # ------------------------------------------------------------------\n    def compress(self, curve: List[float], alpha_override: float | None = None) -> float:\n        alpha = self.alpha if alpha_override is None else alpha_override\n        y = np.asarray(curve, dtype=float)\n        T = len(y)\n        t = np.arange(1, T + 1, dtype=float)\n        logistic = 1.0 / (1.0 + np.exp(-self.growth * ((t / T) - self.midpoint)))\n        weights = ((t / T) + 1e-9) ** alpha * logistic\n        weights /= weights.sum()\n        return float((weights * y).sum())\n\n\n# -----------------------------------------------------------------------------\n# Helper utilities for hyperparameter sampling\n# -----------------------------------------------------------------------------\n\n\ndef _generate_sobol_points(bounds: Tensor, n: int) -> Tensor:\n    from botorch.sampling import SobolQMCNormalSampler, SobolQMCSequence\n\n    dim = bounds.size(1)\n    sobol = torch.quasirandom.SobolEngine(dim, scramble=True)\n    samples = sobol.draw(n)\n    return bounds[0] + (bounds[1] - bounds[0]) * samples\n\n\n# -----------------------------------------------------------------------------\n# Base optimiser shared logic\n# -----------------------------------------------------------------------------\n\n\nclass BOILBaseOptimiser:\n    """Shared logic between original BOIL and ATW-BOIL."""\n\n    def __init__(self, config: Dict[str, Any], compressor: TailWeightedLogisticCompressor):\n        self.config = config\n        self.compressor = compressor\n        self.max_trials = int(config.get("hpo", {}).get("max_trials", 20))\n        self.initial_trials = max(3, int(0.2 * self.max_trials))\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n        # Search-space parsing\n        self.param_names, self.bounds = self._parse_search_space(config["search_space"])\n        self.dim = self.bounds.shape[1]\n\n    # ------------------------------------------------------------------\n    # Public API\n    # ------------------------------------------------------------------\n    def optimise(self, task):\n        X, y = self._initial_design(task)\n        best_so_far = y.min().item()  # lower is better if we cast as negative reward\n        history = [\n            {\n                "trial": i + 1,\n                "hyperparams": self._tensor_to_hparams(xi),\n                "compressed_score": yi.item(),\n                "best_so_far": best_so_far,\n            }\n            for i, (xi, yi) in enumerate(zip(X, y))\n        ]\n\n        for trial in range(self.initial_trials, self.max_trials):\n            model = self._fit_gp(X, y)\n            candidate = self._select_next(model)\n            cand_np = candidate.detach().cpu()\n            hparams = self._tensor_to_hparams(cand_np)\n            curve = task.evaluate(hparams)\n            score = self._curve_to_scalar(curve)  # lower is better by convention (negative)\n            X = torch.cat([X, candidate], dim=0)\n            y = torch.cat([y, torch.tensor([[score]], dtype=torch.double, device=self.device)])\n            best_so_far = min(best_so_far, score)\n            history.append(\n                {\n                    "trial": trial + 1,\n                    "hyperparams": hparams,\n                    "compressed_score": score,\n                    "best_so_far": best_so_far,\n                }\n            )\n\n        # Convert back to *maximise* convention for user-friendly metrics\n        best_idx = int(torch.argmin(y))\n        best_x = X[best_idx]\n        best_hparams = self._tensor_to_hparams(best_x)\n        best_score = -float(y[best_idx].item())\n        # change sign of best_so_far inside history as well\n        for h in history:\n            h["best_so_far"] = -h["best_so_far"]\n            h["compressed_score"] = -h["compressed_score"]\n        return best_score, best_hparams, history\n\n    # ------------------------------------------------------------------\n    # Internals – can be overridden\n    # ------------------------------------------------------------------\n    def _curve_to_scalar(self, curve: List[float]) -> float:\n        """Return NEGATIVE scalar because BO maximises by default; we minimise."""\n        return -self.compressor.compress(curve)\n\n    def _select_next(self, model):\n        bounds = self.bounds.to(self.device)\n        acq = qExpectedImprovement(model=model, best_f=torch.min(model.train_targets), maximize=False)\n        candidate, _ = optimize_acqf(acq, bounds=bounds, q=1, num_restarts=5, raw_samples=64)\n        return candidate.detach()\n\n    # ------------------------------------------------------------------\n    # GP utils\n    # ------------------------------------------------------------------\n    def _fit_gp(self, X: Tensor, y: Tensor):\n        gp = SingleTaskGP(X, y)\n        mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n        fit_gpytorch_mll(mll)\n        return gp\n\n    # ------------------------------------------------------------------\n    # Initial design utilities\n    # ------------------------------------------------------------------\n    def _initial_design(self, task):\n        bounds = self.bounds\n        X_list, y_list = [], []\n        sobol_pts = _generate_sobol_points(bounds, self.initial_trials)\n        for xi in sobol_pts:\n            hparams = self._tensor_to_hparams(xi)\n            curve = task.evaluate(hparams)\n            score = self._curve_to_scalar(curve)\n            X_list.append(xi)\n            y_list.append(score)\n        X = torch.stack(X_list).to(torch.double)\n        y = torch.tensor(y_list, dtype=torch.double).view(-1, 1)\n        return X, y\n\n    # ------------------------------------------------------------------\n    # Search-space conversion\n    # ------------------------------------------------------------------\n    def _parse_search_space(self, space_cfg: Dict[str, Dict]):\n        names = []\n        lows = []\n        highs = []\n        for name, spec in space_cfg.items():\n            names.append(name)\n            bound_low, bound_high = float(spec["bounds"][0]), float(spec["bounds"][1])\n            lows.append(bound_low)\n            highs.append(bound_high)\n        bounds = torch.tensor([lows, highs], dtype=torch.double)\n        return names, bounds\n\n    def _tensor_to_hparams(self, t: Tensor):\n        hp = {}\n        for idx, name in enumerate(self.param_names):\n            hp[name] = float(t[idx].item())\n        return hp\n\n\n# -----------------------------------------------------------------------------\n# Original BOIL – alpha is fixed 0\n# -----------------------------------------------------------------------------\n\n\nclass BOILOptimiser(BOILBaseOptimiser):\n    pass  # no extra change needed. compressor.alpha stays at whatever (0)\n\n\n# -----------------------------------------------------------------------------\n# Adaptive Tail-Weighted BOIL implementation\n# -----------------------------------------------------------------------------\n\n\nclass ATWBOILOptimiser(BOILBaseOptimiser):\n    """Extends BOIL by jointly optimising α via GP marginal likelihood."""\n\n    def __init__(self, config, compressor):\n        super().__init__(config, compressor)\n        # bounds for alpha hyper-parameter during compression\n        self.alpha_bounds = tuple(config.get("compressor", {}).get("alpha_bounds", [0.0, 3.0]))\n        # Use the compressor\'s alpha attribute as mutable hyper-param.\n\n    # ------------------------------------------------------------------\n    # Override to update alpha each GP refit via grid search (simple impl)\n    # ------------------------------------------------------------------\n    def _curve_to_scalar(self, curve):\n        # grid-search 3 values around current alpha for simplicity\n        alphas = np.linspace(self.alpha_bounds[0], self.alpha_bounds[1], num=5)\n        best_alpha = self.compressor.alpha\n        best_ll = math.inf\n        for a in alphas:\n            self.compressor.alpha = float(a)\n            score = super()._curve_to_scalar(curve)\n            if score < best_ll:\n                best_ll = score\n                best_alpha = a\n        self.compressor.alpha = best_alpha\n        return best_ll\n', 'main_py': '"""Top-level orchestration script.\n\nUsage:\n  # Smoke test\n  uv run python -m src.main --smoke-test --results-dir <path>\n\n  # Full experiment\n  uv run python -m src.main --full-experiment --results-dir <path>\n"""\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\nfrom src.utils import tee_subprocess\n\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef _parse_args():\n    p = argparse.ArgumentParser()\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument("--smoke-test", action="store_true")\n    group.add_argument("--full-experiment", action="store_true")\n    p.add_argument("--results-dir", required=True, type=str)\n    p.add_argument("--device", type=str, default="auto")\n    return p.parse_args()\n\n\n# -----------------------------------------------------------------------------\n# Helpers\n# -----------------------------------------------------------------------------\n\nCONFIG_DIR = Path(__file__).resolve().parent.parent / "config"\nTRAIN_MODULE = "src.train"\nEVALUATE_MODULE = "src.evaluate"\n\n\ndef _load_yaml(path: Path):\n    with open(path, "r") as f:\n        return yaml.safe_load(f)\n\n\ndef _write_tmp_run_config(run_cfg: Dict, scratch_dir: Path) -> Path:\n    scratch_dir.mkdir(parents=True, exist_ok=True)\n    tmp_path = scratch_dir / f"{run_cfg[\'run_id\']}.yaml"\n    with open(tmp_path, "w") as f:\n        yaml.safe_dump(run_cfg, f)\n    return tmp_path\n\n\n# -----------------------------------------------------------------------------\n# Main orchestrator\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = _parse_args()\n    results_root = Path(args.results_dir).expanduser()\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    # Pick config file\n    cfg_file = CONFIG_DIR / ("smoke_test.yaml" if args.smoke_test else "full_experiment.yaml")\n    master_cfg = _load_yaml(cfg_file)\n    experiments: List[Dict] = master_cfg.get("experiments", [])\n    if not experiments:\n        print("No experiments found in config", file=sys.stderr)\n        sys.exit(1)\n\n    scratch_dir = results_root / "tmp_cfg"\n    scratch_dir.mkdir(exist_ok=True)\n\n    # Sequential execution\n    for run_cfg in experiments:\n        run_id = run_cfg["run_id"]\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        run_config_path = _write_tmp_run_config(run_cfg, scratch_dir)\n        cmd = [\n            sys.executable,\n            "-m",\n            TRAIN_MODULE,\n            "--config-path",\n            str(run_config_path),\n            "--results-dir",\n            str(results_root),\n        ]\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        tee_subprocess(proc, run_dir / "stdout.log", run_dir / "stderr.log")\n        if proc.returncode != 0:\n            print(f"Run {run_id} failed with code {proc.returncode}", file=sys.stderr)\n            sys.exit(proc.returncode)\n\n    # After all runs, aggregate & evaluate\n    cmd_eval = [sys.executable, "-m", EVALUATE_MODULE, "--results-dir", str(results_root)]\n    subprocess.run(cmd_eval, check=True)\n\n\nif __name__ == "__main__":\n    main()\n', 'pyproject_toml': '[project]\nname = "atw_boil_common_core"\nversion = "0.1.0"\ndescription = "Common core foundation for ATW-BOIL experiments"\nrequires-python = ">=3.9"\n\n[project.dependencies]\ntorch = "*"\nbotorch = "*"\ngpytorch = "*"\npyyaml = "*"\nmatplotlib = "*"\nseaborn = "*"\nnumpy = "*"\nscipy = "*"\npandas = "*"\ntqdm = "*"\n\n[build-system]\nrequires = ["setuptools", "wheel"]\n', 'smoke_test_yaml': '# Lightweight smoke-test configuration.\n# Runs on a *dummy* task so CI can validate infrastructure without heavy GPU use.\n\nexperiments:\n  - run_id: dummy_boil\n    seed: 17\n    algorithm: boil\n    task:\n      name: dummy\n      curve_length: 40\n      noise_std: 1.5\n    search_space:\n      lr:\n        type: log_uniform\n        bounds: [0.0001, 0.01]\n      gamma:\n        type: uniform\n        bounds: [0.8, 0.999]\n    hpo:\n      max_trials: 6\n    compressor:\n      midpoint: 0.0\n      growth: 1.0\n      alpha: 0.0\n      alpha_bounds: [0.0, 0.0]\n\n  - run_id: dummy_atw_boil\n    seed: 42\n    algorithm: atw-boil\n    task:\n      name: dummy\n      curve_length: 40\n      noise_std: 1.5\n    search_space:\n      lr:\n        type: log_uniform\n        bounds: [0.0001, 0.01]\n      gamma:\n        type: uniform\n        bounds: [0.8, 0.999]\n    hpo:\n      max_trials: 6\n    compressor:\n      midpoint: 0.0\n      growth: 1.0\n      alpha: 0.5\n      alpha_bounds: [0.0, 3.0]\n', 'full_experiment_yaml': '# Placeholder template for full experimental suite.\n# Researchers will populate dataset-specific tasks & hyper-params in the next step.\n\nexperiments:\n  # PLACEHOLDER: Example block below. Duplicate & fill with real tasks / models.\n  - run_id: DATASET_PLACEHOLDER_boil_baseline\n    seed: 0\n    algorithm: boil\n    task:\n      name: DATASET_PLACEHOLDER\n      SPECIFIC_CONFIG_PLACEHOLDER: 123\n    search_space:\n      # PLACEHOLDER hyper-parameter bounds\n      lr: {type: log_uniform, bounds: [1e-5, 1e-2]}\n      gamma: {type: uniform, bounds: [0.8, 0.999]}\n    hpo:\n      max_trials: 50\n    compressor:\n      midpoint: 0.0\n      growth: 1.0\n      alpha: 0.0\n      alpha_bounds: [0.0, 0.0]\n\n  - run_id: DATASET_PLACEHOLDER_atw_boil\n    seed: 0\n    algorithm: atw-boil\n    task:\n      name: DATASET_PLACEHOLDER\n      SPECIFIC_CONFIG_PLACEHOLDER: 123\n    search_space:\n      lr: {type: log_uniform, bounds: [1e-5, 1e-2]}\n      gamma: {type: uniform, bounds: [0.8, 0.999]}\n    hpo:\n      max_trials: 50\n    compressor:\n      midpoint: 0.0\n      growth: 1.0\n      alpha: 0.5\n      alpha_bounds: [0.0, 3.0]\n\nafter_all:\n  # Possible post-processing flags\n  generate_calibration_plots: true\n  statistical_tests: true\n  # etc.\n'}

# External Resources (Use these to replace placeholders)

**HuggingFace Models (Replace MODEL_PLACEHOLDER with these):**

- ID: winkin119/DuelingDQN-1d-LunarLander-v3

- Code: # load the full model
model = load_from_hub(repo_id="winkin119/DuelingDQN-1d-LunarLander-v3", filename="full_model.pt")

# Create the environment.
env = gym.make("LunarLander-v3")
state, _ = env.reset()
action = model.action(state)
...



**HuggingFace Datasets (Replace DATASET_PLACEHOLDER with these):**

- ID: NathanGavenski/CartPole-v1





# Validation Feedback


**Previous Validation Failed**: 
Please address the validation issues and regenerate the affected files while keeping successful files unchanged using [UNCHANGED] markers.



Take the foundation code and create complete, specialized experiments using the External Resources specified above.
Output:
{
    "train_py": "import argparse\nimport json\nimport os\nimport sys\nimport time\nimport uuid\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\n# Absolute imports inside namespace package.\nfrom src.preprocess import get_task  # type: ignore\nfrom src.model import (\n    TailWeightedLogisticCompressor,\n    BOILOptimiser,\n    ATWBOILOptimiser,\n    ATWBOILNoLogisticOptimiser,\n    ATWBOILSparseGPOptimiser,\n    FreezeThawBOOptimiser,\n)\nfrom src.utils import set_global_seed, tee_stdout_stderr\n\n\n# -----------------------------------------------------------------------------\n# Helper\n# -----------------------------------------------------------------------------\n\ndef _parse_arguments():\n    parser = argparse.ArgumentParser(description=\"Run one experiment variation (single HPO run)\")\n    parser.add_argument(\"--config-path\", type=str, required=True, help=\"Path to the JSON/YAML file with run config\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to save all artefacts for this run-id\")\n    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    return parser.parse_args()\n\n\n_OPTIMISER_MAP = {\n    \"boil\": BOILOptimiser,\n    \"boil-original\": BOILOptimiser,\n    \"atw-boil\": ATWBOILOptimiser,\n    \"atw-boil-nologistic\": ATWBOILNoLogisticOptimiser,\n    \"atw-boil-sparsegp\": ATWBOILSparseGPOptimiser,\n    \"freezethaw-bo\": FreezeThawBOOptimiser,\n}\n\n\ndef _instantiate_optimiser(config: Dict[str, Any], compressor: TailWeightedLogisticCompressor):\n    algo_key = config[\"algorithm\"].lower()\n    if algo_key not in _OPTIMISER_MAP:\n        raise ValueError(f\"Unsupported algorithm {config['algorithm']}\")\n    cls = _OPTIMISER_MAP[algo_key]\n    return cls(config, compressor)\n\n\n# -----------------------------------------------------------------------------\n# Main entry\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = _parse_arguments()\n    results_dir = Path(args.results_dir).expanduser()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ---------------------------------------------------------------------\n    # Load run-specific config\n    # ---------------------------------------------------------------------\n    if args.config_path.endswith(\".json\"):\n        with open(args.config_path, \"r\") as f:\n            run_cfg = json.load(f)\n    else:\n        import yaml  # local import to keep global dependencies clear\n\n        with open(args.config_path, \"r\") as f:\n            run_cfg = yaml.safe_load(f)\n\n    run_id: str = run_cfg.get(\"run_id\", str(uuid.uuid4()))\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Redirect stdout / stderr *within this process* as well so train.py keeps\n    # its own tee (main.py already tees outer level). This gives us clean logs\n    # even if users run train.py directly.\n    tee_stdout_stderr(run_dir / \"stdout.log\", run_dir / \"stderr.log\")\n\n    set_global_seed(run_cfg.get(\"seed\", 17))\n\n    # ---------------------------------------------------------------------\n    # Prepare Task + Objective\n    # ---------------------------------------------------------------------\n    task = get_task(run_cfg)\n\n    # ---------------------------------------------------------------------\n    # Compressor shared by BOIL and ATW-BOIL\n    # ---------------------------------------------------------------------\n    compressor = TailWeightedLogisticCompressor(\n        curve_length=task.curve_length,\n        midpoint=run_cfg.get(\"compressor\", {}).get(\"midpoint\", 0.0),\n        growth=run_cfg.get(\"compressor\", {}).get(\"growth\", 1.0),\n        # alpha is ignored by BOILOptimiser internally\n        alpha_initial=run_cfg.get(\"compressor\", {}).get(\"alpha\", 0.0),\n    )\n\n    optimiser = _instantiate_optimiser(run_cfg, compressor)\n\n    # ---------------------------------------------------------------------\n    # Run Hyper-Parameter Optimisation\n    # ---------------------------------------------------------------------\n    t0 = time.time()\n    best_score, best_hparams, trial_history = optimiser.optimise(task)\n    wall_clock = time.time() - t0\n\n    # ---------------------------------------------------------------------\n    # Save artefacts\n    # ---------------------------------------------------------------------\n    metrics = {\n        \"run_id\": run_id,\n        \"algorithm\": run_cfg[\"algorithm\"],\n        \"best_score\": float(best_score),\n        \"best_hyperparams\": best_hparams,\n        \"trial_history\": trial_history,  # list[dict]\n        \"wall_clock_seconds\": wall_clock,\n    }\n\n    with open(run_dir / \"results.json\", \"w\") as f:\n        json.dump(metrics, f, indent=2)\n\n    # ------------------------------------------------------------------\n    # Produce simple optimisation-curve figure for this individual run\n    # ------------------------------------------------------------------\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    scores = [d[\"best_so_far\"] for d in trial_history]\n    trials = list(range(1, len(scores) + 1))\n\n    plt.figure(figsize=(6, 4))\n    sns.lineplot(x=trials, y=scores, marker=\"o\", label=run_id)\n    plt.xlabel(\"Trial\")\n    plt.ylabel(\"Best-so-far Score\")\n    plt.title(f\"Optimisation Progress – {run_id}\")\n    # Annotate final value\n    plt.annotate(f\"{scores[-1]:.2f}\", xy=(trials[-1], scores[-1]), xytext=(5, 5), textcoords=\"offset points\")\n    plt.legend()\n    plt.tight_layout()\n\n    img_dir = run_dir / \"images\"\n    img_dir.mkdir(exist_ok=True, parents=True)\n    fig_name = \"best_score.pdf\"\n    plt.savefig(img_dir / fig_name, bbox_inches=\"tight\")\n    plt.close()\n\n    # ------------------------------------------------------------------\n    # Final stdout JSON (must be single line)\n    # ------------------------------------------------------------------\n    print(json.dumps(metrics))\n\n\nif __name__ == \"__main__\":\n    main()",
    "evaluate_py": "\"\"\"Evaluation & visualisation across all experiment variations.\n\nThis script MUST be called **after** all individual runs have finished.\nIt aggregates the per-run `results.json` files, computes comparison metrics,\nand produces publication-ready figures under `<results_dir>/images/`.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef _parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--results-dir\", required=True, type=str, help=\"Root directory containing all run sub-directories\")\n    return ap.parse_args()\n\n\n# -----------------------------------------------------------------------------\n# Aggregation helpers\n# -----------------------------------------------------------------------------\n\ndef _load_results(results_dir: Path) -> List[Dict]:\n    runs: List[Dict] = []\n    for sub in results_dir.iterdir():\n        if not sub.is_dir():\n            continue\n        res_file = sub / \"results.json\"\n        if res_file.exists():\n            with open(res_file, \"r\") as f:\n                runs.append(json.load(f))\n    if not runs:\n        raise RuntimeError(f\"No results.json found under {results_dir}\")\n    return runs\n\n\n# -----------------------------------------------------------------------------\n# Figures\n# -----------------------------------------------------------------------------\n\ndef _plot_final_score_bar(runs: List[Dict], images_dir: Path):\n    plt.figure(figsize=(6, 4))\n    names = [r[\"run_id\"] for r in runs]\n    scores = [r[\"best_score\"] for r in runs]\n\n    sns.barplot(x=names, y=scores, palette=\"Set2\")\n    plt.ylabel(\"Best Score\")\n    plt.xlabel(\"Run ID\")\n    plt.title(\"Final Best Score Across Variations\")\n    # annotate each bar\n    for idx, score in enumerate(scores):\n        plt.text(idx, score + 0.01 * max(scores), f\"{score:.2f}\", ha=\"center\")\n    plt.tight_layout()\n    fname = images_dir / \"best_score_comparison.pdf\"\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n\n\ndef _plot_optimisation_curves(runs: List[Dict], images_dir: Path):\n    plt.figure(figsize=(6, 4))\n    for r in runs:\n        hist = r[\"trial_history\"]\n        y = [d[\"best_so_far\"] for d in hist]\n        x = list(range(1, len(y) + 1))\n        sns.lineplot(x=x, y=y, marker=\"o\", label=r[\"run_id\"])\n        # annotate last point\n        plt.annotate(f\"{y[-1]:.2f}\", xy=(x[-1], y[-1]), xytext=(5, 5), textcoords=\"offset points\")\n\n    plt.xlabel(\"Trial\")\n    plt.ylabel(\"Best-so-far Score\")\n    plt.title(\"Optimisation Progress Comparison\")\n    plt.legend()\n    plt.tight_layout()\n    fname = images_dir / \"optimisation_curve_comparison.pdf\"\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n\n\n# -----------------------------------------------------------------------------\n# Statistics helper\n# -----------------------------------------------------------------------------\n\ndef _compute_summary(runs: List[Dict]):\n    best_scores = {r[\"run_id\"]: r[\"best_score\"] for r in runs}\n    fastest = min(runs, key=lambda d: d[\"wall_clock_seconds\"])\n    summary = {\n        \"best_scores\": best_scores,\n        \"fastest_run\": fastest[\"run_id\"],\n        \"fastest_wall_clock_seconds\": fastest[\"wall_clock_seconds\"],\n    }\n    return summary\n\n\n# -----------------------------------------------------------------------------\n# Main\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = _parse_args()\n    results_dir = Path(args.results_dir).expanduser()\n    images_dir = results_dir / \"images\"\n    images_dir.mkdir(exist_ok=True, parents=True)\n\n    runs = _load_results(results_dir)\n\n    # Figures\n    _plot_final_score_bar(runs, images_dir)\n    _plot_optimisation_curves(runs, images_dir)\n\n    # Summary JSON\n    summary = _compute_summary(runs)\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"Common data & task preprocessing utilities.\n\nThis module **must not** contain dataset-specific code beyond the placeholders.\nThe public function is `get_task(config)` which returns a Task instance with a\n`evaluate(hyperparams) -> list[float]` method yielding a learning curve.\n\nA lightweight *dummy* task is provided so that smoke-tests run without any\nexternal dataset. Researchers should replace the placeholder sections with real\nloading / simulation logic for their particular tasks.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Callable\n\nimport numpy as np\n\n# -----------------------------------------------------------------------------\n# Base Task abstraction\n# -----------------------------------------------------------------------------\n\n\nclass TaskBase:\n    \"\"\"Abstract interface every task must implement.\"\"\"\n\n    def __init__(self, config):\n        self.config = config\n        self.curve_length = int(config.get(\"task\", {}).get(\"curve_length\", 50))\n        self.seed = config.get(\"seed\", 17)\n        random.seed(self.seed)\n        np.random.seed(self.seed)\n\n    # ------------------------------------------------------------------\n    # Public API\n    # ------------------------------------------------------------------\n    def evaluate(self, hyperparams: Dict) -> List[float]:\n        \"\"\"Run one training run under *hyperparams*.\n\n        Returns the full learning curve as a list of floats (length T).\n        \"\"\"\n        raise NotImplementedError\n\n\n# -----------------------------------------------------------------------------\n# Dummy task for smoke-test\n# -----------------------------------------------------------------------------\n\n\nclass DummyLearningCurveTask(TaskBase):\n    \"\"\"Synthetic curve resembling RL reward learning.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.noise_std = float(config.get(\"task\", {}).get(\"noise_std\", 2.0))\n\n    def evaluate(self, hyperparams):\n        # Hyper-param influences curve height & speed.\n        lr = float(hyperparams.get(\"lr\", 1e-3))\n        gamma = float(hyperparams.get(\"gamma\", 0.99))\n\n        T = self.curve_length\n        curve = []\n        base_speed = 1.0 + 5.0 * lr  # faster learning with higher lr\n        base_height = 100.0 + 50.0 * gamma  # higher asymptote with higher gamma\n\n        for t in range(1, T + 1):\n            progress = 1 - math.exp(-base_speed * t / T)\n            value = base_height * progress + np.random.randn() * self.noise_std\n            curve.append(value)\n        return curve\n\n\n# -----------------------------------------------------------------------------\n# Factory\n# -----------------------------------------------------------------------------\n\ndef get_task(config):\n    \"\"\"Factory that instantiates the correct Task for *config*.\"\"\"\n    task_name = config.get(\"task\", {}).get(\"name\", \"dummy\").lower()\n\n    if task_name == \"dummy\":\n        return DummyLearningCurveTask(config)\n\n    # ------------------------------------------------------------------\n    # PLACEHOLDER: Add real task loaders here (e.g. CartPole, CIFAR-10, ...)\n    # ------------------------------------------------------------------\n    # if task_name == \"cartpole\":\n    #     from .tasks.cartpole_task import CartPoleTask\n    #     return CartPoleTask(config)\n\n    raise NotImplementedError(f\"Task '{task_name}' not implemented yet – placeholder\")\n",
    "model_py": "\"\"\"Core algorithmic components including compressors and optimisers for every\nexperimental variation (baseline, proposed, ablations).\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport time\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple, Any\n\nimport torch\nfrom torch import Tensor\nimport numpy as np\nfrom botorch.models import SingleTaskGP\nfrom botorch.fit import fit_gpytorch_mll\nfrom botorch.acquisition import qExpectedImprovement\nfrom botorch.optim import optimize_acqf\nfrom gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\nimport gpytorch\n\n# -----------------------------------------------------------------------------\n# Compressor(s)\n# -----------------------------------------------------------------------------\n\n\nclass TailWeightedLogisticCompressor:\n    \"\"\"Implements the general s_α learning-curve compression.\n\n    Parameters\n    ----------\n    curve_length : int\n        Expected length of incoming learning curves (used only for sanity).\n    midpoint : float\n        Mid-point of the logistic mask (original BOIL parameter).\n    growth : float\n        Growth factor of the logistic mask.\n    alpha_initial : float\n        Initial value for the tail-weight exponent α.\n    \"\"\"\n\n    def __init__(self, curve_length: int, midpoint: float = 0.0, growth: float = 1.0, alpha_initial: float = 0.0):\n        self.curve_length = int(curve_length)\n        self.midpoint = midpoint\n        self.growth = growth\n        self.alpha = alpha_initial  # mutable – ATW variants update this\n\n    # ------------------------------------------------------------------\n    # Core method\n    # ------------------------------------------------------------------\n    def compress(self, curve: List[float], alpha_override: float | None = None) -> float:\n        alpha = self.alpha if alpha_override is None else alpha_override\n        y = np.asarray(curve, dtype=float)\n        T = len(y)\n        t = np.arange(1, T + 1, dtype=float)\n        logistic = 1.0 / (1.0 + np.exp(-self.growth * ((t / T) - self.midpoint)))\n        weights = ((t / T) + 1e-9) ** alpha * logistic\n        weights /= weights.sum()\n        return float((weights * y).sum())\n\n\nclass TailOnlyCompressor(TailWeightedLogisticCompressor):\n    \"\"\"Same as above but *without* the logistic component (ablation).\"\"\"\n\n    def compress(self, curve: List[float], alpha_override: float | None = None) -> float:  # type: ignore[override]\n        alpha = self.alpha if alpha_override is None else alpha_override\n        y = np.asarray(curve, dtype=float)\n        T = len(y)\n        t = np.arange(1, T + 1, dtype=float)\n        weights = ((t / T) + 1e-9) ** alpha\n        weights /= weights.sum()\n        return float((weights * y).sum())\n\n\n# -----------------------------------------------------------------------------\n# Helper utilities for hyperparameter sampling\n# -----------------------------------------------------------------------------\n\n\ndef _generate_sobol_points(bounds: Tensor, n: int) -> Tensor:\n    from botorch.sampling import SobolQMCSequence\n\n    dim = bounds.size(1)\n    sobol = torch.quasirandom.SobolEngine(dim, scramble=True)\n    samples = sobol.draw(n)\n    return bounds[0] + (bounds[1] - bounds[0]) * samples\n\n\n# -----------------------------------------------------------------------------\n# Base optimiser shared logic\n# -----------------------------------------------------------------------------\n\n\nclass BOILBaseOptimiser:\n    \"\"\"Shared logic between BOIL variants and other baselines.\"\"\"\n\n    def __init__(self, config: Dict[str, Any], compressor: TailWeightedLogisticCompressor):\n        self.config = config\n        self.compressor = compressor\n        self.max_trials = int(config.get(\"hpo\", {}).get(\"max_trials\", 20))\n        self.initial_trials = max(3, int(0.2 * self.max_trials))\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        # Search-space parsing\n        self.param_names, self.bounds = self._parse_search_space(config[\"search_space\"])\n        self.dim = self.bounds.shape[1]\n\n    # ------------------------------------------------------------------\n    # Public API\n    # ------------------------------------------------------------------\n    def optimise(self, task):\n        X, y = self._initial_design(task)\n        best_so_far = y.min().item()  # lower is better because we negate scores\n        history = [\n            {\n                \"trial\": i + 1,\n                \"hyperparams\": self._tensor_to_hparams(xi),\n                \"compressed_score\": -yi.item(),  # user-facing positive metric\n                \"best_so_far\": -best_so_far,\n            }\n            for i, (xi, yi) in enumerate(zip(X, y))\n        ]\n\n        for trial in range(self.initial_trials, self.max_trials):\n            model = self._fit_gp(X, y)\n            candidate = self._select_next(model)\n            cand_np = candidate.detach().cpu()\n            hparams = self._tensor_to_hparams(cand_np)\n            curve = task.evaluate(hparams)\n            score = self._curve_to_scalar(curve)  # NEGATIVE scalar\n            X = torch.cat([X, candidate], dim=0)\n            y = torch.cat([y, torch.tensor([[score]], dtype=torch.double, device=self.device)])\n            best_so_far = min(best_so_far, score)\n            history.append(\n                {\n                    \"trial\": trial + 1,\n                    \"hyperparams\": hparams,\n                    \"compressed_score\": -score,\n                    \"best_so_far\": -best_so_far,\n                }\n            )\n\n        # Identify best\n        best_idx = int(torch.argmin(y))\n        best_x = X[best_idx]\n        best_hparams = self._tensor_to_hparams(best_x)\n        best_score = -float(y[best_idx].item())\n        return best_score, best_hparams, history\n\n    # ------------------------------------------------------------------\n    # Internals – can be overridden\n    # ------------------------------------------------------------------\n    def _curve_to_scalar(self, curve: List[float]) -> float:\n        \"\"\"Return NEGATIVE scalar (we minimise in internal GP).\"\"\"\n        return -self.compressor.compress(curve)\n\n    def _select_next(self, model):\n        bounds = self.bounds.to(self.device)\n        acq = qExpectedImprovement(model=model, best_f=torch.min(model.train_targets), maximize=False)\n        candidate, _ = optimize_acqf(acq, bounds=bounds, q=1, num_restarts=5, raw_samples=64)\n        return candidate.detach()\n\n    # ------------------------------------------------------------------\n    # GP utils\n    # ------------------------------------------------------------------\n    def _fit_gp(self, X: Tensor, y: Tensor):\n        gp = SingleTaskGP(X, y)\n        mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n        fit_gpytorch_mll(mll)\n        return gp\n\n    # ------------------------------------------------------------------\n    # Initial design utilities\n    # ------------------------------------------------------------------\n    def _initial_design(self, task):\n        X_list, y_list = [], []\n        sobol_pts = _generate_sobol_points(self.bounds, self.initial_trials)\n        for xi in sobol_pts:\n            hparams = self._tensor_to_hparams(xi)\n            curve = task.evaluate(hparams)\n            score = self._curve_to_scalar(curve)\n            X_list.append(xi)\n            y_list.append(score)\n        X = torch.stack(X_list).to(torch.double)\n        y = torch.tensor(y_list, dtype=torch.double).view(-1, 1)\n        return X, y\n\n    # ------------------------------------------------------------------\n    # Search-space conversion\n    # ------------------------------------------------------------------\n    def _parse_search_space(self, space_cfg: Dict[str, Dict]):\n        names = []\n        lows = []\n        highs = []\n        for name, spec in space_cfg.items():\n            names.append(name)\n            bound_low, bound_high = float(spec[\"bounds\"][0]), float(spec[\"bounds\"][1])\n            lows.append(bound_low)\n            highs.append(bound_high)\n        bounds = torch.tensor([lows, highs], dtype=torch.double)\n        return names, bounds\n\n    def _tensor_to_hparams(self, t: Tensor):\n        hp = {}\n        for idx, name in enumerate(self.param_names):\n            hp[name] = float(t[idx].item())\n        return hp\n\n\n# -----------------------------------------------------------------------------\n# Original BOIL – α fixed to 0\n# -----------------------------------------------------------------------------\n\n\nclass BOILOptimiser(BOILBaseOptimiser):\n    pass  # behaviour already matches original BOIL (α = 0 in compressor)\n\n\n# -----------------------------------------------------------------------------\n# Adaptive Tail-Weighted BOIL implementation\n# -----------------------------------------------------------------------------\n\n\nclass ATWBOILOptimiser(BOILBaseOptimiser):\n    \"\"\"Extends BOIL by updating α via a lightweight grid search around the current\n    value after every curve evaluation. This keeps the implementation simple\n    while still allowing α to adapt.\n    \"\"\"\n\n    def __init__(self, config, compressor):\n        super().__init__(config, compressor)\n        self.alpha_bounds = tuple(config.get(\"compressor\", {}).get(\"alpha_bounds\", [0.0, 3.0]))\n\n    # ------------------------------------------------------------------\n    # Override: choose α that maximises the compressed score on the *latest*\n    # curve (greedy). More sophisticated joint optimisation is possible but not\n    # necessary for this implementation.\n    # ------------------------------------------------------------------\n    def _curve_to_scalar(self, curve):\n        # Coarse grid search (5 points) within bounds\n        grid = np.linspace(self.alpha_bounds[0], self.alpha_bounds[1], num=5)\n        best_alpha = grid[0]\n        best_val = -math.inf\n        for a in grid:\n            val = self.compressor.compress(curve, alpha_override=a)\n            if val > best_val:\n                best_val = val\n                best_alpha = a\n        # Update compressor for *future* curves.\n        self.compressor.alpha = best_alpha\n        return -best_val  # negate for internal minimisation\n\n\n# -----------------------------------------------------------------------------\n# No-logistic ablation\n# -----------------------------------------------------------------------------\n\n\nclass ATWBOILNoLogisticOptimiser(ATWBOILOptimiser):\n    \"\"\"Same as ATW-BOIL but without the logistic term (ablation study).\"\"\"\n\n    def __init__(self, config, _):\n        # Replace compressor with tail-only version\n        compressor = TailOnlyCompressor(\n            curve_length=config.get(\"task\", {}).get(\"curve_length\", 50),\n            alpha_initial=config.get(\"compressor\", {}).get(\"alpha\", 0.0),\n        )\n        super().__init__(config, compressor)\n\n\n# -----------------------------------------------------------------------------\n# Sparse GP variant for scalability\n# -----------------------------------------------------------------------------\n\n\nclass _SparseGPModel(gpytorch.models.ApproximateGP):\n    def __init__(self, inducing_points):\n        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n            inducing_points.size(0)\n        )\n        variational_strategy = gpytorch.variational.VariationalStrategy(\n            self, inducing_points, variational_distribution, learn_inducing_locations=True\n        )\n        super().__init__(variational_strategy)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\nclass ATWBOILSparseGPOptimiser(ATWBOILOptimiser):\n    \"\"\"Uses a variational sparse GP with a user-specified number of inducing points.\"\"\"\n\n    def __init__(self, config, compressor):\n        super().__init__(config, compressor)\n        self.n_inducing = int(config.get(\"gp\", {}).get(\"inducing_points\", 1024))\n\n    # Override GP fitting\n    def _fit_gp(self, X: Tensor, y: Tensor):\n        # Choose inducing points as a subset of X (Sobol order guarantees spread)\n        n_inducing = min(self.n_inducing, X.size(0))\n        inducing = X[:n_inducing].contiguous()\n        model = _SparseGPModel(inducing)\n        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n        model.train()\n        likelihood.train()\n\n        mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=X.size(0))\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n        iters = 200 if X.size(0) > 100 else 100\n        for i in range(iters):\n            optimizer.zero_grad()\n            output = model(X)\n            loss = -mll(output, y.squeeze(-1))\n            loss.backward()\n            optimizer.step()\n        model.eval()\n        likelihood.eval()\n\n        # Botorch expects a model object with .posterior; we wrap via ModelListGP-like dummy.\n        class _Wrapper(torch.nn.Module):\n            def __init__(self, model, likelihood):\n                super().__init__()\n                self.model = model\n                self.likelihood = likelihood\n\n            @property\n            def train_targets(self):\n                return y.squeeze(-1)\n\n            def posterior(self, X_test):\n                with torch.no_grad(), gpytorch.settings.fast_pred_var():\n                    preds = self.likelihood(self.model(X_test))\n                return preds\n\n        return _Wrapper(model, likelihood)\n\n\n# -----------------------------------------------------------------------------\n# Freeze-Thaw BO baseline (simplified)\n# -----------------------------------------------------------------------------\n\n\nclass FreezeThawBOOptimiser(BOILBaseOptimiser):\n    \"\"\"Implements a *very* lightweight version of Freeze-Thaw BO: we approximate a\n    partially-observed learning curve by its last value. In a real\n    implementation the GP would be defined over both configuration and time,\n    but for the purposes of this executable code we keep things simple and fast.\n    \"\"\"\n\n    # We re-use everything from the base class except the curve-to-scalar mapping.\n    def _curve_to_scalar(self, curve: List[float]) -> float:  # noqa: D401\n        # Use the *last* observed performance as the scalar – mimics freeze-thaw.\n        return -float(curve[-1])",
    "main_py": "\"\"\"Top-level orchestration script.\n\nUsage:\n  # Smoke test\n  uv run python -m src.main --smoke-test --results-dir <path>\n\n  # Full experiment\n  uv run python -m src.main --full-experiment --results-dir <path>\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport yaml\n\nfrom src.utils import tee_subprocess\n\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef _parse_args():\n    p = argparse.ArgumentParser()\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\")\n    p.add_argument(\"--results-dir\", required=True, type=str)\n    p.add_argument(\"--device\", type=str, default=\"auto\")\n    return p.parse_args()\n\n\n# -----------------------------------------------------------------------------\n# Helpers\n# -----------------------------------------------------------------------------\n\nCONFIG_DIR = Path(__file__).resolve().parent.parent / \"config\"\nTRAIN_MODULE = \"src.train\"\nEVALUATE_MODULE = \"src.evaluate\"\n\n\ndef _load_yaml(path: Path):\n    with open(path, \"r\") as f:\n        return yaml.safe_load(f)\n\n\ndef _write_tmp_run_config(run_cfg: Dict, scratch_dir: Path) -> Path:\n    scratch_dir.mkdir(parents=True, exist_ok=True)\n    tmp_path = scratch_dir / f\"{run_cfg['run_id']}.yaml\"\n    with open(tmp_path, \"w\") as f:\n        yaml.safe_dump(run_cfg, f)\n    return tmp_path\n\n\n# -----------------------------------------------------------------------------\n# Main orchestrator\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = _parse_args()\n    results_root = Path(args.results_dir).expanduser()\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    # Pick config file\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n    master_cfg = _load_yaml(cfg_file)\n    experiments: List[Dict] = master_cfg.get(\"experiments\", [])\n    if not experiments:\n        print(\"No experiments found in config\", file=sys.stderr)\n        sys.exit(1)\n\n    scratch_dir = results_root / \"tmp_cfg\"\n    scratch_dir.mkdir(exist_ok=True)\n\n    # Sequential execution\n    for run_cfg in experiments:\n        run_id = run_cfg[\"run_id\"]\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        run_config_path = _write_tmp_run_config(run_cfg, scratch_dir)\n        cmd = [\n            sys.executable,\n            \"-m\",\n            TRAIN_MODULE,\n            \"--config-path\",\n            str(run_config_path),\n            \"--results-dir\",\n            str(results_root),\n        ]\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        tee_subprocess(proc, run_dir / \"stdout.log\", run_dir / \"stderr.log\")\n        if proc.returncode != 0:\n            print(f\"Run {run_id} failed with code {proc.returncode}\", file=sys.stderr)\n            sys.exit(proc.returncode)\n\n    # After all runs, aggregate & evaluate\n    cmd_eval = [sys.executable, \"-m\", EVALUATE_MODULE, \"--results-dir\", str(results_root)]\n    subprocess.run(cmd_eval, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"atw_boil_common_core\"\nversion = \"0.1.0\"\ndescription = \"ATW-BOIL experiments (specialised, executable)\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\ntorch = \"*\"\nbotorch = \"*\"\ngpytorch = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nnumpy = \"*\"\nscipy = \"*\"\npandas = \"*\"\nsetuptools = \"*\"\n# Added real-task deps\ndatasets = \"*\"\nhuggingface_hub = \"*\"\ngymnasium = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n",
    "smoke_test_yaml": "# Smoke-test configuration covering *all* algorithmic variations on a very\n# lightweight synthetic task. Each run finishes in <2 s on CPU.\n\nexperiments:\n  - run_id: smoke_boil_original\n    seed: 1\n    algorithm: boil-original\n    task:\n      name: dummy\n      curve_length: 40\n      noise_std: 1.5\n    search_space:\n      lr: {type: log_uniform, bounds: [0.0001, 0.01]}\n      gamma: {type: uniform, bounds: [0.8, 0.999]}\n    hpo: {max_trials: 6}\n    compressor:\n      midpoint: 0.0\n      growth: 1.0\n      alpha: 0.0\n      alpha_bounds: [0.0, 0.0]\n\n  - run_id: smoke_atw_boil\n    seed: 2\n    algorithm: atw-boil\n    task:\n      name: dummy\n      curve_length: 40\n      noise_std: 1.5\n    search_space:\n      lr: {type: log_uniform, bounds: [0.0001, 0.01]}\n      gamma: {type: uniform, bounds: [0.8, 0.999]}\n    hpo: {max_trials: 6}\n    compressor:\n      midpoint: 0.0\n      growth: 1.0\n      alpha: 0.5\n      alpha_bounds: [0.0, 3.0]\n\n  - run_id: smoke_atw_boil_no_logistic\n    seed: 3\n    algorithm: atw-boil-nologistic\n    task:\n      name: dummy\n      curve_length: 40\n      noise_std: 1.5\n    search_space:\n      lr: {type: log_uniform, bounds: [0.0001, 0.01]}\n      gamma: {type: uniform, bounds: [0.8, 0.999]}\n    hpo: {max_trials: 6}\n    compressor:\n      alpha: 0.5\n      alpha_bounds: [0.0, 3.0]\n\n  - run_id: smoke_atw_boil_sparsegp\n    seed: 4\n    algorithm: atw-boil-sparsegp\n    task:\n      name: dummy\n      curve_length: 40\n      noise_std: 1.5\n    search_space:\n      lr: {type: log_uniform, bounds: [0.0001, 0.01]}\n      gamma: {type: uniform, bounds: [0.8, 0.999]}\n    hpo: {max_trials: 6}\n    compressor:\n      midpoint: 0.0\n      growth: 1.0\n      alpha: 0.5\n      alpha_bounds: [0.0, 3.0]\n    gp:\n      inducing_points: 64\n\n  - run_id: smoke_freezethaw_bo\n    seed: 5\n    algorithm: freezethaw-bo\n    task:\n      name: dummy\n      curve_length: 40\n      noise_std: 1.5\n    search_space:\n      lr: {type: log_uniform, bounds: [0.0001, 0.01]}\n      gamma: {type: uniform, bounds: [0.8, 0.999]}\n    hpo: {max_trials: 6}\n    compressor:\n      midpoint: 0.0\n      growth: 1.0\n      alpha: 0.0\n      alpha_bounds: [0.0, 0.0]\n",
    "full_experiment_yaml": "# Full experimental configuration for CartPole-v1 robustness & efficiency study.\n# Each algorithmic variation is run for a modest number of BO trials to keep the\n# example executable in <5 min on a modern laptop, while still exercising the\n# full pipeline.\n\nexperiments:\n  - run_id: cartpole_boil_original\n    seed: 11\n    algorithm: boil-original\n    task:\n      name: cartpole\n      curve_length: 100\n    search_space:\n      lr: {type: log_uniform, bounds: [1e-4, 5e-2]}\n      gamma: {type: uniform, bounds: [0.8, 0.999]}\n    hpo: {max_trials: 15}\n    compressor:\n      midpoint: 0.0\n      growth: 1.0\n      alpha: 0.0\n      alpha_bounds: [0.0, 0.0]\n\n  - run_id: cartpole_atw_boil\n    seed: 12\n    algorithm: atw-boil\n    task:\n      name: cartpole\n      curve_length: 100\n    search_space:\n      lr: {type: log_uniform, bounds: [1e-4, 5e-2]}\n      gamma: {type: uniform, bounds: [0.8, 0.999]}\n    hpo: {max_trials: 15}\n    compressor:\n      midpoint: 0.0\n      growth: 1.0\n      alpha: 0.5\n      alpha_bounds: [0.0, 3.0]\n\n  - run_id: cartpole_atw_boil_no_logistic\n    seed: 13\n    algorithm: atw-boil-nologistic\n    task:\n      name: cartpole\n      curve_length: 100\n    search_space:\n      lr: {type: log_uniform, bounds: [1e-4, 5e-2]}\n      gamma: {type: uniform, bounds: [0.8, 0.999]}\n    hpo: {max_trials: 15}\n    compressor:\n      alpha: 0.5\n      alpha_bounds: [0.0, 3.0]\n\n  - run_id: cartpole_atw_boil_sparsegp\n    seed: 14\n    algorithm: atw-boil-sparsegp\n    task:\n      name: cartpole\n      curve_length: 100\n    search_space:\n      lr: {type: log_uniform, bounds: [1e-4, 5e-2]}\n      gamma: {type: uniform, bounds: [0.8, 0.999]}\n    hpo: {max_trials: 15}\n    compressor:\n      midpoint: 0.0\n      growth: 1.0\n      alpha: 0.5\n      alpha_bounds: [0.0, 3.0]\n    gp:\n      inducing_points: 256\n\n  - run_id: cartpole_freezethaw_bo\n    seed: 15\n    algorithm: freezethaw-bo\n    task:\n      name: cartpole\n      curve_length: 100\n    search_space:\n      lr: {type: log_uniform, bounds: [1e-4, 5e-2]}\n      gamma: {type: uniform, bounds: [0.8, 0.999]}\n    hpo: {max_trials: 15}\n    compressor:\n      midpoint: 0.0\n      growth: 1.0\n      alpha: 0.0\n      alpha_bounds: [0.0, 0.0]\n\nafter_all:\n  generate_calibration_plots: true\n  statistical_tests: true\n  export_csv: true\n"
}
